{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3f2e654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have more than 1 gpu, use device '0' or '1' to assign to a gpu\n",
    "# import os\n",
    "# os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import gym\n",
    "import numpy as np\n",
    "# from reinforce_tf2 import Agent\n",
    "# from utils import plotLearning\n",
    "import tensorflow as tf\n",
    "# from networks import PolicyGradientNetwork\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense \n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "895649b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradientNetwork(keras.Model):\n",
    "    def __init__(self, n_actions, fc1_dims=256, fc2_dims=256):\n",
    "        super(PolicyGradientNetwork, self).__init__()\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        self.fc1 = Dense(self.fc1_dims, activation='relu')\n",
    "        self.fc2 = Dense(self.fc2_dims, activation='relu')\n",
    "        self.pi = Dense(n_actions, activation='softmax')\n",
    "\n",
    "    def call(self, state):\n",
    "        value = self.fc1(state)\n",
    "        value = self.fc2(value)\n",
    "\n",
    "        pi = self.pi(value)\n",
    "\n",
    "        return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a02144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, alpha=0.003, gamma=0.99, n_actions=4,\n",
    "                 layer1_size=256, layer2_size=256):\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.lr = alpha\n",
    "        self.n_actions = n_actions\n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "        self.policy = PolicyGradientNetwork(n_actions=n_actions)\n",
    "        self.policy.compile(optimizer=Adam(learning_rate=self.lr))\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        state = tf.convert_to_tensor([observation], dtype=tf.float32)\n",
    "        probs = self.policy(state)\n",
    "        action_probs = tfp.distributions.Categorical(probs=probs)\n",
    "        action = action_probs.sample()\n",
    "#         print(action.numpy()[0])\n",
    "        return action.numpy()[0]\n",
    "\n",
    "    def store_transition(self, observation, action, reward):\n",
    "        self.state_memory.append(observation)\n",
    "        self.action_memory.append(action)\n",
    "        self.reward_memory.append(reward)\n",
    "\n",
    "    def learn(self):\n",
    "        actions = tf.convert_to_tensor(self.action_memory, dtype=tf.float32)\n",
    "        rewards = np.array(self.reward_memory)\n",
    "\n",
    "        G = np.zeros_like(rewards)\n",
    "        for t in range(len(rewards)):\n",
    "            G_sum = 0\n",
    "            discount = 1\n",
    "            for k in range(t, len(rewards)):\n",
    "                G_sum += rewards[k] * discount\n",
    "                discount *= self.gamma\n",
    "            G[t] = G_sum\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = 0\n",
    "            for idx, (g, state) in enumerate(zip(G, self.state_memory)):\n",
    "                state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "                probs = self.policy(state)\n",
    "                action_probs = tfp.distributions.Categorical(probs=probs)\n",
    "                log_prob = action_probs.log_prob(actions[idx])\n",
    "                loss += -g * tf.squeeze(log_prob)\n",
    "\n",
    "        gradient = tape.gradient(loss, self.policy.trainable_variables)\n",
    "        self.policy.optimizer.apply_gradients(zip(gradient, self.policy.trainable_variables))\n",
    "\n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "650b07c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0 score: 34.0 average score 34.0\n",
      "episode:  1 score: 24.0 average score 29.0\n",
      "episode:  2 score: 26.0 average score 28.0\n",
      "episode:  3 score: 10.0 average score 23.5\n",
      "episode:  4 score: 17.0 average score 22.2\n",
      "episode:  5 score: 16.0 average score 21.2\n",
      "episode:  6 score: 11.0 average score 19.7\n",
      "episode:  7 score: 13.0 average score 18.9\n",
      "episode:  8 score: 15.0 average score 18.4\n",
      "episode:  9 score: 18.0 average score 18.4\n",
      "episode:  10 score: 28.0 average score 19.3\n",
      "episode:  11 score: 18.0 average score 19.2\n",
      "episode:  12 score: 32.0 average score 20.2\n",
      "episode:  13 score: 20.0 average score 20.1\n",
      "episode:  14 score: 33.0 average score 21.0\n",
      "episode:  15 score: 23.0 average score 21.1\n",
      "episode:  16 score: 23.0 average score 21.2\n",
      "episode:  17 score: 26.0 average score 21.5\n",
      "episode:  18 score: 16.0 average score 21.2\n",
      "episode:  19 score: 32.0 average score 21.8\n",
      "episode:  20 score: 16.0 average score 21.5\n",
      "episode:  21 score: 56.0 average score 23.0\n",
      "episode:  22 score: 19.0 average score 22.9\n",
      "episode:  23 score: 18.0 average score 22.7\n",
      "episode:  24 score: 23.0 average score 22.7\n",
      "episode:  25 score: 43.0 average score 23.5\n",
      "episode:  26 score: 44.0 average score 24.2\n",
      "episode:  27 score: 18.0 average score 24.0\n",
      "episode:  28 score: 14.0 average score 23.7\n",
      "episode:  29 score: 14.0 average score 23.3\n",
      "episode:  30 score: 18.0 average score 23.2\n",
      "episode:  31 score: 27.0 average score 23.3\n",
      "episode:  32 score: 33.0 average score 23.6\n",
      "episode:  33 score: 20.0 average score 23.5\n",
      "episode:  34 score: 32.0 average score 23.7\n",
      "episode:  35 score: 29.0 average score 23.9\n",
      "episode:  36 score: 13.0 average score 23.6\n",
      "episode:  37 score: 28.0 average score 23.7\n",
      "episode:  38 score: 67.0 average score 24.8\n",
      "episode:  39 score: 78.0 average score 26.1\n",
      "episode:  40 score: 23.0 average score 26.0\n",
      "episode:  41 score: 35.0 average score 26.3\n",
      "episode:  42 score: 51.0 average score 26.8\n",
      "episode:  43 score: 27.0 average score 26.8\n",
      "episode:  44 score: 57.0 average score 27.5\n",
      "episode:  45 score: 32.0 average score 27.6\n",
      "episode:  46 score: 83.0 average score 28.8\n",
      "episode:  47 score: 25.0 average score 28.7\n",
      "episode:  48 score: 39.0 average score 28.9\n",
      "episode:  49 score: 20.0 average score 28.7\n",
      "episode:  50 score: 71.0 average score 29.6\n",
      "episode:  51 score: 25.0 average score 29.5\n",
      "episode:  52 score: 38.0 average score 29.6\n",
      "episode:  53 score: 40.0 average score 29.8\n",
      "episode:  54 score: 73.0 average score 30.6\n",
      "episode:  55 score: 43.0 average score 30.8\n",
      "episode:  56 score: 39.0 average score 31.0\n",
      "episode:  57 score: 22.0 average score 30.8\n",
      "episode:  58 score: 35.0 average score 30.9\n",
      "episode:  59 score: 26.0 average score 30.8\n",
      "episode:  60 score: 45.0 average score 31.0\n",
      "episode:  61 score: 51.0 average score 31.4\n",
      "episode:  62 score: 67.0 average score 31.9\n",
      "episode:  63 score: 54.0 average score 32.3\n",
      "episode:  64 score: 65.0 average score 32.8\n",
      "episode:  65 score: 52.0 average score 33.1\n",
      "episode:  66 score: 38.0 average score 33.1\n",
      "episode:  67 score: 59.0 average score 33.5\n",
      "episode:  68 score: 31.0 average score 33.5\n",
      "episode:  69 score: 78.0 average score 34.1\n",
      "episode:  70 score: 51.0 average score 34.4\n",
      "episode:  71 score: 95.0 average score 35.2\n",
      "episode:  72 score: 58.0 average score 35.5\n",
      "episode:  73 score: 30.0 average score 35.4\n",
      "episode:  74 score: 42.0 average score 35.5\n",
      "episode:  75 score: 40.0 average score 35.6\n",
      "episode:  76 score: 68.0 average score 36.0\n",
      "episode:  77 score: 28.0 average score 35.9\n",
      "episode:  78 score: 58.0 average score 36.2\n",
      "episode:  79 score: 89.0 average score 36.9\n",
      "episode:  80 score: 42.0 average score 36.9\n",
      "episode:  81 score: 81.0 average score 37.5\n",
      "episode:  82 score: 112.0 average score 38.3\n",
      "episode:  83 score: 146.0 average score 39.6\n",
      "episode:  84 score: 54.0 average score 39.8\n",
      "episode:  85 score: 40.0 average score 39.8\n",
      "episode:  86 score: 76.0 average score 40.2\n",
      "episode:  87 score: 108.0 average score 41.0\n",
      "episode:  88 score: 99.0 average score 41.6\n",
      "episode:  89 score: 125.0 average score 42.6\n",
      "episode:  90 score: 79.0 average score 43.0\n",
      "episode:  91 score: 48.0 average score 43.0\n",
      "episode:  92 score: 81.0 average score 43.4\n",
      "episode:  93 score: 82.0 average score 43.8\n",
      "episode:  94 score: 84.0 average score 44.3\n",
      "episode:  95 score: 48.0 average score 44.3\n",
      "episode:  96 score: 143.0 average score 45.3\n",
      "episode:  97 score: 55.0 average score 45.4\n",
      "episode:  98 score: 189.0 average score 46.9\n",
      "episode:  99 score: 87.0 average score 47.3\n",
      "episode:  100 score: 102.0 average score 48.0\n",
      "episode:  101 score: 103.0 average score 48.7\n",
      "episode:  102 score: 134.0 average score 49.8\n",
      "episode:  103 score: 96.0 average score 50.7\n",
      "episode:  104 score: 32.0 average score 50.8\n",
      "episode:  105 score: 67.0 average score 51.3\n",
      "episode:  106 score: 147.0 average score 52.7\n",
      "episode:  107 score: 110.0 average score 53.7\n",
      "episode:  108 score: 73.0 average score 54.2\n",
      "episode:  109 score: 57.0 average score 54.6\n",
      "episode:  110 score: 41.0 average score 54.8\n",
      "episode:  111 score: 36.0 average score 55.0\n",
      "episode:  112 score: 43.0 average score 55.1\n",
      "episode:  113 score: 110.0 average score 56.0\n",
      "episode:  114 score: 102.0 average score 56.6\n",
      "episode:  115 score: 59.0 average score 57.0\n",
      "episode:  116 score: 41.0 average score 57.2\n",
      "episode:  117 score: 177.0 average score 58.7\n",
      "episode:  118 score: 149.0 average score 60.0\n",
      "episode:  119 score: 100.0 average score 60.7\n",
      "episode:  120 score: 47.0 average score 61.0\n",
      "episode:  121 score: 87.0 average score 61.3\n",
      "episode:  122 score: 112.0 average score 62.3\n",
      "episode:  123 score: 55.0 average score 62.6\n",
      "episode:  124 score: 48.0 average score 62.9\n",
      "episode:  125 score: 32.0 average score 62.8\n",
      "episode:  126 score: 38.0 average score 62.7\n",
      "episode:  127 score: 78.0 average score 63.3\n",
      "episode:  128 score: 112.0 average score 64.3\n",
      "episode:  129 score: 102.0 average score 65.2\n",
      "episode:  130 score: 46.0 average score 65.5\n",
      "episode:  131 score: 120.0 average score 66.4\n",
      "episode:  132 score: 45.0 average score 66.5\n",
      "episode:  133 score: 75.0 average score 67.0\n",
      "episode:  134 score: 157.0 average score 68.3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m     observation \u001b[38;5;241m=\u001b[39m observation_\n\u001b[0;32m     18\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m---> 19\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m score_history\u001b[38;5;241m.\u001b[39mappend(score)\n\u001b[0;32m     22\u001b[0m agent\u001b[38;5;241m.\u001b[39mlearn()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rl\\lib\\site-packages\\gym\\core.py:286\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender(mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rl\\lib\\site-packages\\gym\\core.py:286\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender(mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rl\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:258\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen\u001b[38;5;241m.\u001b[39mblit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 258\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    259\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    agent = Agent(alpha=0.0005,  gamma=0.99, n_actions=2)\n",
    "\n",
    "    env = gym.make('CartPole-v1')\n",
    "    score_history = []\n",
    "\n",
    "    num_episodes = 2000\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        done = False\n",
    "        score = 0\n",
    "        observation = env.reset()\n",
    "        while not done:\n",
    "            action = agent.choose_action(observation)\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            agent.store_transition(observation, action, reward)\n",
    "            observation = observation_\n",
    "            score += reward\n",
    "            env.render()\n",
    "        score_history.append(score)\n",
    "\n",
    "        agent.learn()\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "        print('episode: ', i,'score: %.1f' % score,\n",
    "            'average score %.1f' % avg_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
