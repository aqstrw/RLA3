{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1b9NyNwg4b8D"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "from random import sample, randint\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import os\n",
    "import glob\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gejnn6vFAwVw",
    "outputId": "5e5f6ced-e8b6-4d85-9552-948480576069"
   },
   "source": [
    "cwd = os.getcwd()\n",
    "if cwd == \"/content\":\n",
    "    from google.colab import drive\n",
    "print(\"cwd is :\", cwd)\n",
    "def printdocs(pname):\n",
    "    exec(\"print(\"+str(pname)+\".__doc__)\")\n",
    "\n",
    "if cwd == \"/content\":\n",
    "    drive.mount('/content/drive')\n",
    "    !ls /content/drive/MyDrive/data/RLA3_data\n",
    "    %cd /content/drive/MyDrive/data/RLA3_data\n",
    "    from Helper import argmax, softmax\n",
    "else:\n",
    "    print(\"cwd is :\", cwd)\n",
    "\n",
    "print(\"argmax docs :\", argmax.__doc__)\n",
    "print(\"softmax docs :\", softmax.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g1JScJte1jMz"
   },
   "outputs": [],
   "source": [
    "# if no helper available, use this cell\n",
    "\n",
    "# def softmax(x, temp):\n",
    "#     ''' Computes the softmax of vector x with temperature parameter 'temp' '''\n",
    "#     x = x / temp # scale by temperature\n",
    "#     z = x - max(x) # substract max to prevent overflow of softmax \n",
    "#     return np.exp(z)/np.sum(np.exp(z)) # compute softmax\n",
    "\n",
    "# def argmax(x):\n",
    "#     ''' Own variant of np.argmax with random tie breaking '''\n",
    "#     try:\n",
    "#         return np.random.choice(np.where(x == np.max(x))[0])\n",
    "#     except:\n",
    "#         return np.argmax(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rlLRT7-30grx"
   },
   "outputs": [],
   "source": [
    "class policy_network(keras.Model):\n",
    "    def __init__(self, n_states, n_actions, ):\n",
    "        super().__init__()\n",
    "        self.n_actions = n_actions\n",
    "        self.ip = keras.layers.Flatten(input_shape = n_states)\n",
    "        self.l1 = keras.layers.Dense(24, kernel_initializer = tf.keras.initializers.HeUniform(seed=None), activation=\"relu\")\n",
    "        self.l2 = keras.layers.Dense(24, kernel_initializer = tf.keras.initializers.HeUniform(seed=None), activation=\"relu\")\n",
    "        self.l3 = keras.layers.Dense(24, kernel_initializer = tf.keras.initializers.HeUniform(seed=None), activation=\"relu\")\n",
    "        self.op = keras.layers.Dense(n_actions, activation=\"softmax\")\n",
    "#     def __init__(self, n_actions, fc1_dims=256, fc2_dims=256):\n",
    "#         super(policy_network, self).__init__()\n",
    "#         self.fc1_dims = fc1_dims\n",
    "#         self.fc2_dims = fc2_dims\n",
    "#         self.n_actions = n_actions\n",
    "\n",
    "#         self.fc1 = Dense(self.fc1_dims, activation='relu')\n",
    "#         self.fc2 = Dense(self.fc2_dims, activation='relu')\n",
    "#         self.pi = Dense(n_actions, activation='softmax')\n",
    "\n",
    "    def call(self, state, ):\n",
    "        fp = self.ip(state)\n",
    "        fp = self.l1(fp)\n",
    "        fp = self.l2(fp)\n",
    "        fp = self.l3(fp)\n",
    "        policy = self.op(fp)\n",
    "        return policy\n",
    "\n",
    "#     def call(self, state):\n",
    "#         value = self.fc1(state)\n",
    "#         value = self.fc2(value)\n",
    "\n",
    "#         pi = self.pi(value)\n",
    "\n",
    "#         return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uJw89W-9U93b"
   },
   "outputs": [],
   "source": [
    "class agent():\n",
    "    def __init__(self, n_actions, n_states, lr = 0.003, gamma = 0.99,  ):\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.n_actions = n_actions\n",
    "        self.n_states = n_states\n",
    "        print(\"action space shape : {}\".format(self.n_actions))\n",
    "        print(\"state space shape : {}\".format(self.n_states))\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.pi = policy_network(n_states = self.n_states, n_actions = n_actions)                    # check\n",
    "#         self.pi = policy_network(n_actions = n_actions)\n",
    "        self.pi.compile(optimizer = keras.optimizers.Adam(learning_rate = self.lr))                  # check\n",
    "\n",
    "    def get_act(self, state, ):\n",
    "        '''takes a state and returns a tensor of action categorical probabilities'''\n",
    "        state_t = tf.convert_to_tensor([state], dtype = tf.float32)\n",
    "        probs = self.pi(state_t)\n",
    "        cat_probs = tfp.distributions.Categorical(probs = probs)\n",
    "        action = cat_probs.sample()\n",
    "#         print(action.numpy()[0])\n",
    "        \n",
    "        return action.numpy()[0]\n",
    "    \n",
    "#         state = tf.convert_to_tensor([observation], dtype=tf.float32)\n",
    "#         probs = self.policy(state)\n",
    "#         action_probs = tfp.distributions.Categorical(probs=probs)\n",
    "#         action = action_probs.sample()\n",
    "#         print(action.numpy()[0])\n",
    "#         return action.numpy()\n",
    "\n",
    "    def remember(self, state, action, reward, ):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    def learn(self):\n",
    "        actions_t = tf.convert_to_tensor(self.actions, dtype = tf.float32)\n",
    "        rewards_arr = np.array(self.rewards)\n",
    "\n",
    "        returns = np.zeros_like(rewards_arr)\n",
    "        for state_id in range(returns.shape[-1]):\n",
    "            returns_ds = 0\n",
    "            discount_factor = 1\n",
    "            for state_id_ds in range(state_id, returns.shape[-1]):\n",
    "                returns_ds += rewards_arr[state_id_ds]*discount_factor\n",
    "                discount_factor *= self.gamma\n",
    "                \n",
    "            returns[state_id] = returns_ds\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = 0\n",
    "            for state_id, (return_g, state_g) in enumerate(zip(returns, self.states)):\n",
    "                state_g = tf.convert_to_tensor([state_g], dtype = tf.float32)\n",
    "                probabilities = self.pi(state_g)\n",
    "                action_probabilities = tfp.distributions.Categorical(probs = probabilities)\n",
    "                log_probabilities = action_probabilities.log_prob(actions_t[state_id])\n",
    "                loss += -return_g * tf.squeeze(log_probabilities)\n",
    "\n",
    "        grad = tape.gradient(loss, self.pi.trainable_variables)\n",
    "        self.pi.optimizer.apply_gradients(zip(grad, self.pi.trainable_variables))\n",
    "\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfUIvANo0fgx",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import your agent\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # initialise environment\n",
    "    env = gym.make('CartPole-v1')\n",
    "\n",
    "    # initialize agent\n",
    "    agent_007 = agent(lr = 0.001, gamma = 0.99, n_actions = env.action_space.n, n_states = env.observation_space.shape)\n",
    "\n",
    "    ep_num = 1000\n",
    "    score_hist = []\n",
    "\n",
    "    # episode loop\n",
    "    for i in range(ep_num):\n",
    "        done = False\n",
    "        score = 0\n",
    "        s = env.reset()\n",
    "        \n",
    "        # step loop\n",
    "        while not done:\n",
    "            a = agent_007.get_act(s)\n",
    "            s_next,reward,done,_ = env.step(a)\n",
    "            agent_007.remember(s,a,reward)\n",
    "            s = s_next\n",
    "            score += reward\n",
    "#             env.render()\n",
    "        score_hist.append(score)\n",
    "\n",
    "        # make agent learn\n",
    "        agent_007.learn()\n",
    "        avg_score = np.mean(score_hist[-100:])\n",
    "        print('episode: ', i,'score: %.1f' % score,\n",
    "            'average score %.1f' % avg_score)\n",
    "        \n",
    "#         print()\n",
    "        \n",
    "    # plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bWmYNh8J0uOX"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "convenience functions\n",
    "'''\n",
    "\n",
    "def single_plot(data,title = \"title\",xsize = 7, ysize = 7,xaxis = \"xaxis\", yaxis = \"yaxis\", yscale = \"linear\"):\n",
    "    '''\n",
    "    single_plot(data,title = \"title\",xsize = 7, ysize = 7,xaxis = \"xaxis\", yaxis = \"yaxis\", yscale = \"linear\"):\n",
    "    plot a figure with a single subplot using 'data' data and 'title' of size (xsize X ysize)\n",
    "    '''\n",
    "    fig,axs = plt.subplots(1,1,figsize = (xsize,ysize))\n",
    "    # axs.imshow(im[frame_num],norm = mpl.colors.LogNorm())\n",
    "    axs.plot(data)\n",
    "    axs.set_xlabel(xaxis)\n",
    "    axs.set_ylabel(yaxis)\n",
    "    axs.set_yscale(yscale)\n",
    "    axs.set_title(title)\n",
    "    return axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "id": "xIEtj5gB7PKJ",
    "outputId": "6828fc89-df24-4d90-fa02-74b6afa10c80"
   },
   "outputs": [],
   "source": [
    "# create a model\n",
    "def get_model(ip_shape,lr,op_shape,summary = True):\n",
    "    '''\n",
    "    get_model(ip_shape,lr,op_shape,summary = True):\n",
    "    creates and returns a model and prints it's summary based on summary flag\n",
    "    '''\n",
    "    \n",
    "    \n",
    "\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=ip_shape))\n",
    "    model.add(keras.layers.Dense(24, kernel_initializer = tf.keras.initializers.HeUniform(seed=None), activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(24, kernel_initializer = tf.keras.initializers.HeUniform(seed=None), activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(24, kernel_initializer = tf.keras.initializers.HeUniform(seed=None), activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(op_shape, activation=\"linear\"))\n",
    "\n",
    "    # compile model\n",
    "    model.compile(loss=\"mean_squared_error\",optimizer=keras.optimizers.Adam(learning_rate=lr),metrics=[\"accuracy\"])\n",
    "    if summary == True:\n",
    "        print(model.summary())\n",
    "    return(model)\n",
    "\n",
    "# test the model\n",
    "test_model = get_model(np.array([4,]),0.01,np.array([2,]))\n",
    "tf.keras.utils.plot_model(test_model, to_file = \"test_model_plot.png\", show_shapes = True)\n",
    "\n",
    "\n",
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "test_model_graph = cv2.imread(\"test_model_plot.png\")\n",
    "cv2_imshow(test_model_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MsrTgXwahrDc"
   },
   "outputs": [],
   "source": [
    "# experience replay deque class\n",
    "class experience_deque:\n",
    "    '''\n",
    "    __init__(self, max_len):\n",
    "            #initialise max buffer length\n",
    "        self.deque_size = max_len\n",
    "        \n",
    "        # initialise buffer for live deque length\n",
    "        self.live_ds = 0\n",
    "        \n",
    "        # initialise experience buffers\n",
    "        self.s_experience = deque(maxlen = self.deque_size)\n",
    "        self.s_next_experience = deque(maxlen = self.deque_size)\n",
    "        self.a_experience = deque(maxlen = self.deque_size)\n",
    "        self.r_experience = deque(maxlen = self.deque_size)\n",
    "        self.d_experience = deque(maxlen = self.deque_size)\n",
    "    \n",
    "    # methods:\n",
    "    add_experience(self,s,a,r,s_next,done):\n",
    "    get_batch(self, batch_size):\n",
    "    '''\n",
    "    def __init__(self, max_len):\n",
    "        '''initialisation'''\n",
    "        #initialise max buffer length\n",
    "        self.deque_size = max_len\n",
    "        \n",
    "        # initialise buffer for live deque length\n",
    "        self.live_ds = 0\n",
    "        \n",
    "        # initialise experience buffers\n",
    "        self.s_experience = deque(maxlen = self.deque_size)\n",
    "        self.s_next_experience = deque(maxlen = self.deque_size)\n",
    "        self.a_experience = deque(maxlen = self.deque_size)\n",
    "        self.r_experience = deque(maxlen = self.deque_size)\n",
    "        self.d_experience = deque(maxlen = self.deque_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def add_experience(self,s,a,r,s_next,done):\n",
    "        '''\n",
    "        add_experience(self,s,a,r,s_next,done)\n",
    "        add an experience to the deques\n",
    "        '''\n",
    "        self.s_experience.append(s)\n",
    "        self.s_next_experience.append(s_next)\n",
    "        self.a_experience.append(a)\n",
    "        self.r_experience.append(r)\n",
    "        self.d_experience.append(done)\n",
    "        \n",
    "        #update live deque size\n",
    "        self.live_ds = len(self.s_experience)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    def get_batch(self, batch_size):\n",
    "        '''\n",
    "        get_batch(self, batch_size):\n",
    "        generate random samples from experiences\n",
    "        returns them\n",
    "        '''\n",
    "        # warn that deque is not full\n",
    "        if self.live_ds < self.deque_size:\n",
    "            if self.live_ds%1000 == 0:\n",
    "                print(self.live_ds)\n",
    "                \n",
    "#             print(\"deque is not full, current size is : \", self.live_ds)\n",
    "#             if batch_size > self.live_ds:\n",
    "#                 print(\"batch size bigger than live deque size (bs,lds): \", batch_size, self.live_ds)\n",
    "#             else:\n",
    "#                 print(\"sampling from incomplete deque (bs,lds): \", batch_size, self.live_ds)\n",
    "        \n",
    "        # get random indices\n",
    "        ind = sample(range(self.live_ds), batch_size)\n",
    "        \n",
    "        # sample from all deques\n",
    "        s_sampled = np.asarray(self.s_experience)[ind]\n",
    "        s_next_sampled = np.asarray(self.s_next_experience)[ind]\n",
    "        a_sampled = np.asarray(self.a_experience)[ind]\n",
    "        r_sampled = np.asarray(self.r_experience)[ind]\n",
    "        d_sampled = np.asarray(self.d_experience)[ind]\n",
    "        \n",
    "        return (s_sampled,s_next_sampled,a_sampled,r_sampled,d_sampled)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PTJ_9zdLWtvx"
   },
   "outputs": [],
   "source": [
    "class DQNagent:\n",
    "    '''\n",
    "    __init__(self, n_states, n_actions, learning_rate, gamma, max_len,\n",
    "    er = True, tn = True, conv = False, summary = True, verbose = 0):\n",
    "    Sets up a model and provides handy methods to interact with it\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, env, buffer, live_model, gamma, target_model, er = True,\n",
    "                 TN = True, summary = True, verbose = 2, ):\n",
    "        '''Iniitialization function for class DQNagent, read the __docs__'''\n",
    "        \n",
    "        # used for \n",
    "        self.TN = TN\n",
    "        self.buffer = buffer\n",
    "        self.live_model = live_model\n",
    "        if TN:\n",
    "            self.target_model = target_model\n",
    "        self.n_states = env.observation_space.shape\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.gamma = gamma\n",
    "#         self.deque_size = max_len\n",
    "        self.verbose = verbose\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    def select_action(self, state, policy='egreedy', epsilon=None, temp=None):\n",
    "        '''\n",
    "        select_action(self, s, policy='egreedy', epsilon=None, temp=None):\n",
    "        selects action based on policy specified\n",
    "        returns action\n",
    "        '''\n",
    "        state = state.reshape(1,4)\n",
    "        if policy == 'egreedy':\n",
    "            if epsilon is None:\n",
    "                raise KeyError(\"Provide an epsilon\")\n",
    "                \n",
    "            # TO DO: Add own code\n",
    "            exploit = np.random.choice([0,1],p = [epsilon,1-epsilon])\n",
    "            if exploit:\n",
    "#                 print(q[0])\n",
    "                a = argmax((self.live_model.predict(state))[0])\n",
    "#                 print(\"exploiting: \",a)\n",
    "            else:\n",
    "                a = np.random.randint(0,self.n_actions) # Replace this with correct action selection\n",
    "#                 print(\"exploring: \",a)\n",
    "                \n",
    "#         elif policy == 'softmax':\n",
    "#             if temp is None:\n",
    "#                 raise KeyError(\"Provide a temperature\")\n",
    "                \n",
    "#             # TO DO: Add own code\n",
    "#             a = np.random.randint(0,self.n_actions) # Replace this with correct action selection\n",
    "#             print(\"action selected :\", a)\n",
    "        return a\n",
    "                \n",
    "\n",
    "    \n",
    "    def update_er_tn(self,batch_of_ss, batch_of_as, batch_of_sns, batch_of_rs,batch_of_ds, batch_size):\n",
    "        '''\n",
    "        update_er_tn(self,batch):\n",
    "        perform a Q-learning update\n",
    "        '''\n",
    "        if not self.TN:\n",
    "            print(\"wrong call\")\n",
    "\n",
    "        # get target Q values for the batch\n",
    "        q_next_batch = self.target_model.predict(batch_of_sns)\n",
    "        \n",
    "        # calculate targets and assign done rewards\n",
    "        G1 = self.live_model.predict(batch_of_ss)\n",
    "        cat_boa = to_categorical(batch_of_as,num_classes = 2)\n",
    "        cat_inv_boa = to_categorical(np.invert(batch_of_as),num_classes = 2)\n",
    "        G1 = G1 * cat_inv_boa\n",
    "        G2 = batch_of_rs + ( self.gamma * np.max(q_next_batch, axis = 1) )\n",
    "        G2 = np.where(batch_of_ds == True, batch_of_rs, G2)\n",
    "        G2 = G2.reshape(G2.shape[0],1)\n",
    "        G2 = G2 * cat_boa\n",
    "        G_batch = G1+G2\n",
    "\n",
    "        \n",
    "        # update live_network\n",
    "        history = self.live_model.fit(batch_of_ss,G_batch, batch_size = batch_size, verbose = 0)\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def update_er(self,batch_of_ss, batch_of_as, batch_of_sns, batch_of_rs,batch_of_ds, batch_size):\n",
    "        '''\n",
    "        update_er(self,batch):\n",
    "        perform a Q-learning update\n",
    "        '''\n",
    "        if self.TN:\n",
    "            print(\"wrong call\")\n",
    "        # get target Q values for the batch\n",
    "        q_next_batch = self.live_model.predict(batch_of_sns)\n",
    "        \n",
    "        # calculate targets and assign done rewards\n",
    "        G1 = self.live_model.predict(batch_of_ss)\n",
    "        cat_boa = to_categorical(batch_of_as,num_classes = 2)\n",
    "        cat_inv_boa = to_categorical(np.invert(batch_of_as),num_classes = 2)\n",
    "        G1 = G1 * cat_inv_boa\n",
    "        G2 = batch_of_rs + ( self.gamma * np.max(q_next_batch, axis = 1) )\n",
    "        G2 = np.where(batch_of_ds == True, batch_of_rs, G2)\n",
    "        G2 = G2.reshape(G2.shape[0],1)\n",
    "        G2 = G2 * cat_boa\n",
    "        G_batch = G1+G2\n",
    "\n",
    "        \n",
    "        # update live_network\n",
    "        history = self.live_model.fit(batch_of_ss,G_batch, batch_size = batch_size, verbose = 0)\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def synch_weights(self):\n",
    "        '''\n",
    "        synch_weights(self):\n",
    "        synchronises target model weights\n",
    "        '''\n",
    "        self.target_model.set_weights(self.live_model.get_weights())\n",
    "        print(\"weights synched\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9JFZAIwXwu3"
   },
   "outputs": [],
   "source": [
    "\n",
    "def decay_eps(epsilon = 1, min_epsilon = 0.01, decay_rate = 0.95):\n",
    "    '''\n",
    "    decay_eps(max_epsilon = 1, min_epsilon = 0.01, decay_rate = 0.995):\n",
    "    decay the epsilon value\n",
    "    '''\n",
    "#     if epsilon<0.2:\n",
    "#         decay_rate = 0.995\n",
    "    epsilon *= decay_rate\n",
    "    return max(epsilon, min_epsilon)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ghMw9CtnXzdP"
   },
   "outputs": [],
   "source": [
    "# main loop for a single run, loop to average over runs is not included here\n",
    "\n",
    "def Qlearn(learning_rate, epsilon, buffer_size, n_eps, max_timesteps,min_batch_size, synch_weight_freq, decay_epsilon = True, TN = True):\n",
    "    '''\n",
    "        Qlearn(learning_rate, epsilon, buffer_size, n_eps, max_timesteps,min_batch_size, synch_weight_freq, decay_epsilon = True, TN = True):\n",
    "        single run and collect kpis\n",
    "    '''\n",
    "    \n",
    "    # initialise environment\n",
    "    env = gym.make('CartPole-v1')\n",
    "    \n",
    "    # create buffers for kpi\n",
    "    cum_reward_per_ep = [] # list of final rewards [n_eps]\n",
    "\n",
    "    # initialise networks\n",
    "    live_net = get_model(env.observation_space.shape,learning_rate,env.action_space.n)\n",
    "    \n",
    "    if TN:\n",
    "        target_net = get_model(env.observation_space.shape,learning_rate,env.action_space.n)\n",
    "    \n",
    "    # initialise buffers\n",
    "    buffer = experience_deque(max_len = buffer_size)\n",
    "    \n",
    "    # initialise agent\n",
    "#         def __init__(self, env, buffer, live_model, target_model,  gamma, batch_size, er = True,\n",
    "#                  tn = True, summary = True, verbose = 2):\n",
    "    if TN:\n",
    "        agent = DQNagent(env, buffer, live_net, gamma, target_model = target_net, TN = TN)\n",
    "    else:\n",
    "        agent = DQNagent(env, buffer, live_net, gamma, target_model = None, TN = TN)\n",
    "    \n",
    "    # count for target net update\n",
    "    step_count = 0\n",
    "    \n",
    "    # loop over eps\n",
    "    for ep_num in range(n_eps):\n",
    "        print(\"staring ep: \", ep_num)\n",
    "        s = env.reset()\n",
    "        rewards = []\n",
    "        cum_reward = 0\n",
    "        ep_list = []\n",
    "        \n",
    "#         if ep_num == 24:\n",
    "#                 print (\"\\n\\n\\n\\n\\n\\n training interval changed to 100 steps \\n\\n\\n\\n\\n\\n\")\n",
    "#                 synch_weight_freq = 100\n",
    "#         if ep_num == 50:\n",
    "#                 print (\"\\n\\n\\n\\n\\n\\n training interval changed to 1000 steps \\n\\n\\n\\n\\n\\n\")\n",
    "#                 synch_weight_freq = 1000\n",
    "        \n",
    "        # loop over timesteps\n",
    "        for step in range(max_timesteps):\n",
    "            \n",
    "                \n",
    "            step_count += 1\n",
    "#             env.render()\n",
    "            \n",
    "            # select action\n",
    "            a = agent.select_action(state = s,policy = 'egreedy', epsilon = epsilon)\n",
    "\n",
    "            \n",
    "            # play a step\n",
    "            s_next,reward,done,_ = env.step(a)\n",
    "            cum_reward += reward\n",
    "            \n",
    "            # save experience\n",
    "            buffer.add_experience(s,a,reward,s_next,done)\n",
    "            \n",
    "            calc_batch_size = min(int(0.7*buffer.live_ds),500)\n",
    "        \n",
    "            if buffer.live_ds >= min_batch_size:\n",
    "#                 print(\"buffer min reached\")\n",
    "#             if step = 500\n",
    "                \n",
    "                # get batch from memory\n",
    "                s_exp_batch,s_next_exp_batch,a_exp_batch,r_exp_batch,d_exp_batch = buffer.get_batch(calc_batch_size)\n",
    "\n",
    "                # run a DQN training loop\n",
    "                if TN:\n",
    "#                     print(\"TN training\")\n",
    "                    history = agent.update_er_tn(s_exp_batch,a_exp_batch,s_next_exp_batch,r_exp_batch,d_exp_batch,calc_batch_size)\n",
    "#                     print(\"loss = \",history.history[\"loss\"][0])\n",
    "                else:\n",
    "#                     print(\"training\")\n",
    "                    history = agent.update_er(s_exp_batch,a_exp_batch,s_next_exp_batch,r_exp_batch,d_exp_batch,calc_batch_size)\n",
    "                    #                 print(\"loss = \",history.history[\"loss\"][0])\n",
    "                if TN:\n",
    "                    # copy weights to target_network\n",
    "                    if step_count%synch_weight_freq == 0:\n",
    "                        agent.synch_weights()\n",
    "\n",
    "\n",
    "\n",
    "            # check if done\n",
    "            if done:\n",
    "#                 print(\"done = \",done)\n",
    "                print(\"ep = \", ep_num, \"    epsilon = \", epsilon, \"    steps = \", step,\"    ep reward = \",cum_reward)\n",
    "                break\n",
    "                \n",
    "                \n",
    "        \n",
    "            # set new state\n",
    "            s = s_next\n",
    "#             print(step)\n",
    "            \n",
    "            \n",
    "            # timestep loop ends\n",
    "                        \n",
    "            \n",
    "        # end of episode calculations\n",
    "        \n",
    "        # collect kpi\n",
    "        cum_reward_per_ep.append(cum_reward)\n",
    "        \n",
    "        \n",
    "        # Decay probability of taking random action\n",
    "        if decay_epsilon:\n",
    "            epsilon = decay_eps(epsilon,eps_min)\n",
    "            \n",
    "        \n",
    "        # check if done\n",
    "        if ep_num>benchmark_averaging_eps:\n",
    "            avg_10ep_cum_rewards = np.mean(cum_reward_per_ep[-benchmark_averaging_eps:])\n",
    "            print(\"avg 20 ep rewards : \",avg_10ep_cum_rewards)\n",
    "#             if avg_10ep_cum_rewards >= benchmark:\n",
    "#                 print(\"\\n\\n\\n Rewards have converged to a value above the benchmark score (\",benchmark,\") \\n\\n\\n\")\n",
    "#                 break\n",
    "                \n",
    "#                 extend = input('another episode? (y/n)')\n",
    "#                 if extend == 'y':\n",
    "#                     continue\n",
    "#                 else:\n",
    "#                     break\n",
    "                    \n",
    "        ep_list.append(ep_num)\n",
    "                \n",
    "    return (cum_reward_per_ep, avg_10ep_cum_rewards, ep_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BuIM9yEdX3ze"
   },
   "outputs": [],
   "source": [
    "# save to file\n",
    "\n",
    "def safe(fname = \"test.csv\"):\n",
    "    try:\n",
    "        fig = plt.figure(figsize = (14,7))\n",
    "        list_y = []\n",
    "        for i,y in enumerate(cum_rewards):\n",
    "            y = np.array(y)\n",
    "            y = np.pad(y,(0,300-y.shape[0]),mode = 'constant',constant_values = (np.nan))\n",
    "            list_y.append(y)\n",
    "            np.savetxt(fname, list_y, delimiter = \",\" )\n",
    "    #         print(y.shape)\n",
    "            plt.plot(range(y.shape[0]),y,label = r\"run \"+str(i+1))\n",
    "        plt.title(\"TN individual\")\n",
    "        plt.xlabel(\"Rewards\")\n",
    "        plt.ylabel(\"Episodes\")\n",
    "        list_y = np.array(list_y)\n",
    "        print(list_y.shape)\n",
    "        fig = plt.figure(figsize = (14,7))\n",
    "        plt.plot(range(300), np.nanmean(list_y, axis = 0))\n",
    "        plt.xlabel(\"Rewards\")\n",
    "        plt.ylabel(\"Episodes\")\n",
    "        plt.title(\"TN individual\")\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPHokiLK8KKe"
   },
   "outputs": [],
   "source": [
    "# setup run parameters\n",
    "\n",
    "n_eps = 100                  # max number of eps\n",
    "buffer_size = 10000\n",
    "synch_weight_freq = 10\n",
    "learning_rate = 0.001\n",
    "gamma = 0.995                 # discount factor\n",
    "epsilon = 1\n",
    "min_batch_size = 32              # size of the replay sample\n",
    "# min_buff = 1000\n",
    "max_timesteps = 500           # max steps an episode can last\n",
    "eps_min = 0.01\n",
    "num_runs = 10\n",
    "benchmark_averaging_eps = 20  # how many eps to average over to compare benchmark\n",
    "benchmark = 350 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRfRD0Sl72Mv",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "# print(physical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5enQdsVsai8N"
   },
   "outputs": [],
   "source": [
    "# call to run\n",
    "# buffers for kpis\n",
    "cum_rewards = []\n",
    "averages_at_end = []\n",
    "list_of_ep_nums = []\n",
    "\n",
    "# loop for averaging over 8 runs\n",
    "for run in range(num_runs):\n",
    "#     learning_rate, epsilon, buffer_size, n_eps, max_timesteps,min_batch_size\n",
    "    cum_rewards_run, averages_at_end_run, list_of_ep_nums = Qlearn(learning_rate, epsilon, buffer_size,\n",
    "                                                                   n_eps, max_timesteps, min_batch_size, synch_weight_freq,\n",
    "                                                                   TN = True)\n",
    "    cum_rewards.append(cum_rewards_run)\n",
    "    averages_at_end.append(averages_at_end_run)\n",
    "    list_of_ep_nums.append(run)\n",
    "    safe()\n",
    "\n",
    "# summarise results\n",
    "print(np.shape(cum_rewards))\n",
    "print(np.shape(averages_at_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HhxmI4aCOtex",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.plot(range(150),cum_rewards)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "cleanup_wip.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
